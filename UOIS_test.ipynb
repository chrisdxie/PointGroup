{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # for debugging GPU stuff\n",
    "import time, random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import open3d  # Need to import this before torch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "sys.path.append('/home/chrisxie/projects/ssc/')\n",
    "import util.flowlib as flowlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config file\n",
    "cfg_file = '/home/chrisxie/local_installations/PointGroup/config/pointgroup_TOD.yaml'\n",
    "from util.config import get_parser_notebook\n",
    "get_parser_notebook(cfg_file=cfg_file, pretrain_path=None)\n",
    "\n",
    "from util.config import cfg\n",
    "from util.log import logger\n",
    "import util.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "#     global result_dir\n",
    "#     result_dir = os.path.join(cfg.exp_path, 'result', 'nmst{}_scoret{}_npointt{}'.format(cfg.TEST_NMS_THRESH, cfg.TEST_SCORE_THRESH, cfg.TEST_NPOINT_THRESH), cfg.split)\n",
    "#     backup_dir = os.path.join(result_dir, 'backup_files')\n",
    "#     os.makedirs(backup_dir, exist_ok=True)\n",
    "#     os.makedirs(os.path.join(result_dir, 'predicted_masks'), exist_ok=True)\n",
    "#     os.system('cp test.py {}'.format(backup_dir))\n",
    "#     os.system('cp {} {}'.format(cfg.model_dir, backup_dir))\n",
    "#     os.system('cp {} {}'.format(cfg.dataset_dir, backup_dir))\n",
    "#     os.system('cp {} {}'.format(cfg.config, backup_dir))\n",
    "# TODO(chrisdxie): Set directory to write results to: ~/projects/ssc/external\n",
    "\n",
    "    global semantic_label_idx\n",
    "    semantic_label_idx = [0, 1, 2]\n",
    "\n",
    "    logger.info(cfg)\n",
    "\n",
    "    random.seed(cfg.test_seed)\n",
    "    np.random.seed(cfg.test_seed)\n",
    "    torch.manual_seed(cfg.test_seed)\n",
    "    torch.cuda.manual_seed_all(cfg.test_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(ious, scores, threshold):\n",
    "    ixs = scores.argsort()[::-1]\n",
    "    pick = []\n",
    "    while len(ixs) > 0:\n",
    "        i = ixs[0]\n",
    "        pick.append(i)\n",
    "        iou = ious[i, ixs[1:]]\n",
    "        remove_ixs = np.where(iou > threshold)[0] + 1\n",
    "        ixs = np.delete(ixs, remove_ixs)\n",
    "        ixs = np.delete(ixs, 0)\n",
    "    return np.array(pick, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "\n",
    "\n",
    "def imwrite_indexed(filename,array):\n",
    "    \"\"\" Save indexed png with palette.\"\"\"\n",
    "\n",
    "    palette_abspath = '/home/chrisxie/projects/random_stuff/palette.txt' # hard-coded filepath\n",
    "    color_palette = np.loadtxt(palette_abspath, dtype=np.uint8).reshape(-1,3)\n",
    "\n",
    "    if np.atleast_3d(array).shape[2] != 1:\n",
    "        raise Exception(\"Saving indexed PNGs requires 2D array.\")\n",
    "\n",
    "    im = Image.fromarray(array)\n",
    "    im.putpalette(color_palette.ravel())\n",
    "    im.save(filename, format='PNG')\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "\n",
    "# reconstruct image\n",
    "def inpaint_with_valid_mask(values, valid_mask, H=480, W=640):\n",
    "    \"\"\"Use valid_mask to inpaint values back into a HxW image.\"\"\"\n",
    "    \n",
    "    if torch.is_tensor(values):\n",
    "        values = values.cpu().numpy()\n",
    "        \n",
    "    if values.ndim == 1:\n",
    "        values = values[..., None]\n",
    "        channels = 1\n",
    "    else:\n",
    "        channels = values.shape[-1]\n",
    "        \n",
    "    img = np.zeros((H * W, channels), dtype=np.float32)\n",
    "    img[valid_mask] = values\n",
    "    \n",
    "    if channels == 1:  # Return a [H, W] image instead of [H, W, 1]\n",
    "        return img.reshape((H, W))\n",
    "    else:\n",
    "        return img.reshape((H, W, channels))\n",
    "\n",
    "    \n",
    "def inpaint_cluster_img(clusters, valid_mask, H=480, W=640):\n",
    "    cluster_num = 2  # OBJECT starts here\n",
    "    instances_img = np.zeros((H, W), dtype=np.uint8)\n",
    "    for i in range(clusters.shape[0]):\n",
    "        temp = inpaint_with_valid_mask(clusters[i], valid_mask).astype(np.uint8)\n",
    "        assert np.all(~np.logical_and(instances_img > 0, temp > 0)), 'uh oh, the clusters overlap'\n",
    "#         if np.sum(temp) < 500:\n",
    "#             continue\n",
    "        instances_img += temp * cluster_num\n",
    "        cluster_num += 1\n",
    "    return instances_img\n",
    "    \n",
    "    \n",
    "def subplotter(images, max_plots_per_row=4, fig_index_start=1):\n",
    "    \"\"\"Plot images side by side.\n",
    "    \n",
    "    Args:\n",
    "        images: an Iterable of [H, W, C] np.arrays. If images is\n",
    "            a dictionary, the values are assumed to be the arrays,\n",
    "            and the keys are strings which will be titles.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig_index = fig_index_start\n",
    "    \n",
    "    num_plots = len(images)\n",
    "    num_rows = int(np.ceil(num_plots / max_plots_per_row))\n",
    "\n",
    "    for row in range(num_rows):\n",
    "\n",
    "        fig = plt.figure(fig_index, figsize=(max_plots_per_row*5, 5))\n",
    "        fig_index += 1\n",
    "\n",
    "        for j in range(max_plots_per_row):\n",
    "\n",
    "            ind = row*max_plots_per_row + j\n",
    "            if ind >= num_plots:\n",
    "                break\n",
    "\n",
    "            plt.subplot(1, max_plots_per_row, j+1)\n",
    "            if type(images) == dict:\n",
    "                title = list(images.keys())[ind]\n",
    "                image = images[title]\n",
    "                plt.title(title)\n",
    "            else:\n",
    "                image = images[ind]\n",
    "            plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### init\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### get model version and data version\n",
    "exp_name = cfg.config.split('/')[-1][:-5]\n",
    "print(exp_name)\n",
    "model_name = exp_name.split('_')[0]\n",
    "print(model_name)\n",
    "data_name = exp_name.split('_')[1]\n",
    "print(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### model\n",
    "logger.info('=> creating model ...')\n",
    "if model_name == 'pointgroup':\n",
    "    from model.pointgroup.pointgroup import PointGroup as Network\n",
    "    from model.pointgroup.pointgroup import model_fn_decorator\n",
    "else:\n",
    "    print(\"Error: no model - \" + model_name)\n",
    "    exit(0)\n",
    "model = Network(cfg)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "logger.info('cuda available: {}'.format(use_cuda))\n",
    "assert use_cuda\n",
    "model = model.cuda()\n",
    "\n",
    "# logger.info(model)\n",
    "logger.info('#classifier parameters: {}'.format(sum([x.nelement() for x in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### model_fn (criterion)\n",
    "model_fn = model_fn_decorator(test=True)\n",
    "\n",
    "##### load model\n",
    "# utils.checkpoint_restore(model,\n",
    "#                          cfg.exp_path,\n",
    "#                          cfg.config.split('/')[-1][:-5],\n",
    "#                          use_cuda,\n",
    "#                          cfg.test_epoch)\n",
    "# resume from the latest epoch, or specify the epoch to restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### dataset\n",
    "if cfg.dataset == 'TOD':\n",
    "    if data_name == 'TOD':\n",
    "        import data.TOD\n",
    "        data.TOD = reload(data.TOD)\n",
    "        dataset = data.TOD.Dataset()\n",
    "    else:\n",
    "        print(\"Error: no data loader - \" + data_name)\n",
    "        exit(0)\n",
    "elif cfg.dataset == 'OCID':\n",
    "    if data_name == 'OCID':\n",
    "        import data.OCID\n",
    "        data.OCID = reload(data.OCID)\n",
    "        dataset = data.OCID.Dataset()\n",
    "    else:\n",
    "        print(\"Error: no data loader - \" + data_name)\n",
    "        exit(0)\n",
    "elif cfg.dataset == 'OSD':\n",
    "    if data_name == 'OSD':\n",
    "        import data.OSD\n",
    "        data.OSD = reload(data.OSD)\n",
    "        dataset = data.OSD.Dataset()\n",
    "    else:\n",
    "        print(\"Error: no data loader - \" + data_name)\n",
    "dataset.testLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Test loop\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "H = 480; W = 640\n",
    "save_dir = '/home/chrisxie/projects/ssc/external/'\n",
    "if data_name == 'OCID':\n",
    "    save_dir = os.path.join(save_dir, 'OCID_results')\n",
    "elif data_name == 'OSD':\n",
    "    save_dir = os.path.join(save_dir, 'OSD_results')\n",
    "elif data_name == 'TOD':\n",
    "    save_dir = os.path.join(save_dir, 'TODv5_results', 'test_set')\n",
    "else:\n",
    "    raise NotImplementedError(f\"Testing on {data_name} not implemented...\")\n",
    "save_dir = os.path.join(save_dir, 'PointGroup')\n",
    "\n",
    "# num_iters_trained = cfg.test_epoch * len(dataset.train_data_loader)\n",
    "num_iters_trained = 300000  # Force it to cluster\n",
    "with torch.no_grad():\n",
    "    model = model.eval()\n",
    "    start = time.time()\n",
    "\n",
    "    matches = {}\n",
    "    for i, batch in tqdm(enumerate(dataset.test_data_loader)):\n",
    "        \n",
    "        N = batch['feats'].shape[0] \n",
    "\n",
    "        start1 = time.time()\n",
    "        preds = model_fn(batch, model, num_iters_trained)\n",
    "        end1 = time.time() - start1\n",
    "\n",
    "        ##### get predictions (#1 semantic_pred, pt_offsets; #2 scores, proposals_pred)\n",
    "        semantic_scores = preds['semantic']  # (N, nClass) float32, cuda\n",
    "        semantic_pred = semantic_scores.max(1)[1]  # (N) long, cuda\n",
    "\n",
    "        pt_offsets = preds['pt_offsets']    # (N, 3), float32, cuda\n",
    "\n",
    "        scores = preds['score']   # (nProposal, 1) float, cuda\n",
    "        scores_pred = torch.sigmoid(scores.view(-1))\n",
    "        \n",
    "        proposals_idx, proposals_offset = preds['proposals']\n",
    "        # proposals_idx: (sumNPoint, 2), int, cpu, dim 0 for cluster_id, dim 1 for corresponding point idxs in N\n",
    "        # proposals_offset: (nProposal + 1), int, cpu\n",
    "        proposals_pred = torch.zeros((proposals_offset.shape[0] - 1, N), dtype=torch.int, device=scores_pred.device) # (nProposal, N), int, cuda\n",
    "        proposals_pred[proposals_idx[:, 0].long(), proposals_idx[:, 1].long()] = 1\n",
    "\n",
    "        semantic_id = torch.tensor(semantic_label_idx, device=scores_pred.device)[semantic_pred[proposals_idx[:, 1][proposals_offset[:-1].long()].long()]] # (nProposal), long\n",
    "        \n",
    "        ##### score threshold\n",
    "        score_mask = (scores_pred > cfg.TEST_SCORE_THRESH)\n",
    "        scores_pred = scores_pred[score_mask]\n",
    "        proposals_pred = proposals_pred[score_mask]\n",
    "        semantic_id = semantic_id[score_mask]\n",
    "\n",
    "        ##### npoint threshold\n",
    "        proposals_pointnum = proposals_pred.sum(1)\n",
    "        npoint_mask = (proposals_pointnum > cfg.TEST_NPOINT_THRESH)\n",
    "        scores_pred = scores_pred[npoint_mask]\n",
    "        proposals_pred = proposals_pred[npoint_mask]\n",
    "        semantic_id = semantic_id[npoint_mask]\n",
    "        \n",
    "       \n",
    "        ##### nms\n",
    "        if semantic_id.shape[0] == 0:\n",
    "            pick_idxs = np.empty(0)\n",
    "        else:\n",
    "            proposals_pred_f = proposals_pred.float()  # (nProposal, N), float, cuda\n",
    "            intersection = torch.mm(proposals_pred_f, proposals_pred_f.t())  # (nProposal, nProposal), float, cuda\n",
    "            proposals_pointnum = proposals_pred_f.sum(1)  # (nProposal), float, cuda\n",
    "            proposals_pn_h = proposals_pointnum.unsqueeze(-1).repeat(1, proposals_pointnum.shape[0])\n",
    "            proposals_pn_v = proposals_pointnum.unsqueeze(0).repeat(proposals_pointnum.shape[0], 1)\n",
    "            cross_ious = intersection / (proposals_pn_h + proposals_pn_v - intersection)\n",
    "            pick_idxs = non_max_suppression(cross_ious.cpu().numpy(), scores_pred.cpu().numpy(), cfg.TEST_NMS_THRESH)  # int, (nCluster, N)\n",
    "        clusters = proposals_pred[pick_idxs]\n",
    "        cluster_scores = scores_pred[pick_idxs]\n",
    "        cluster_semantic_id = semantic_id[pick_idxs]\n",
    "        \n",
    "        nclusters = clusters.shape[0]\n",
    "        \n",
    "        ##### Inpaint\n",
    "        instances_imgs = np.zeros((batch['offsets'].shape[0]-1, H, W), dtype=np.uint8)\n",
    "        for img_num in range(batch['offsets'].shape[0]-1):\n",
    "            valid_mask = batch['valid_mask'][img_num * H*W : (img_num + 1) * H*W].numpy()\n",
    "            instances_imgs[img_num] = inpaint_cluster_img(\n",
    "                clusters[:, batch['offsets'][img_num] : batch['offsets'][img_num+1]],\n",
    "                valid_mask)\n",
    "        \n",
    "        ##### Save \n",
    "        for i, path in enumerate(batch['label_abs_path']):\n",
    "            file_path = os.path.join(save_dir, path.rsplit('/', 1)[0])\n",
    "            if not os.path.exists(file_path):\n",
    "                os.makedirs(file_path)\n",
    "            file_name = os.path.join(file_path, path.rsplit('/', 1)[1].rsplit('.', 1)[0] + '.png')\n",
    "            imwrite_indexed(file_name, instances_imgs[i].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legacy Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_iter = iter(dataset.test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(d_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "\n",
    "H = 480; W = 640\n",
    "valid_mask = batch['valid_mask'][\n",
    "        img_num * H*W : (img_num + 1) * H*W].numpy()\n",
    "# plt.imshow(valid_mask.reshape(H,W), vmin=0, vmax=1)\n",
    "rgb_vals = batch['feats'][batch['offsets'][img_num] : batch['offsets'][img_num+1]]\n",
    "rgb_img = inpaint_with_valid_mask(rgb_vals, valid_mask)\n",
    "\n",
    "plot_dict = {'rgb': normalize(rgb_img)}\n",
    "if 'instance_labels' in batch:\n",
    "    gt_instances_img = inpaint_with_valid_mask(batch['instance_labels'][batch['offsets'][img_num] : batch['offsets'][img_num+1]],\n",
    "                                               valid_mask)\n",
    "    print(np.unique(gt_instances_img))\n",
    "    gt_instances_img[gt_instances_img == cfg.ignore_label] = 0\n",
    "    \n",
    "    plot_dict['instance_labels'] = gt_instances_img\n",
    "\n",
    "subplotter(plot_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_iters_trained = cfg.test_epoch * len(dataset.train_data_loader)\n",
    "num_iters_trained = 300000\n",
    "with torch.no_grad():\n",
    "    start1 = time.time()\n",
    "    preds = model_fn(batch, model, num_iters_trained)\n",
    "    end1 = time.time() - start1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "N = batch['feats'].shape[0]\n",
    "\n",
    "##### get predictions (#1 semantic_pred, pt_offsets; #2 scores, proposals_pred)\n",
    "semantic_scores = preds['semantic']  # (N, nClass) float32, cuda\n",
    "semantic_pred = semantic_scores.max(1)[1]  # (N) long, cuda\n",
    "\n",
    "pt_offsets = preds['pt_offsets']    # (N, 3), float32, cuda\n",
    "\n",
    "scores = preds['score']   # (nProposal, 1) float, cuda\n",
    "scores_pred = torch.sigmoid(scores.view(-1))\n",
    "\n",
    "proposals_idx, proposals_offset = preds['proposals']\n",
    "# proposals_idx: (sumNPoint, 2), int, cpu, dim 0 for cluster_id, dim 1 for corresponding point idxs in N\n",
    "# proposals_offset: (nProposal + 1), int, cpu\n",
    "proposals_pred = torch.zeros((proposals_offset.shape[0] - 1, N), dtype=torch.int, device=scores_pred.device) # (nProposal, N), int, cuda\n",
    "proposals_pred[proposals_idx[:, 0].long(), proposals_idx[:, 1].long()] = 1\n",
    "\n",
    "semantic_id = torch.tensor(semantic_label_idx, device=scores_pred.device)[semantic_pred[proposals_idx[:, 1][proposals_offset[:-1].long()].long()]] # (nProposal), long\n",
    "\n",
    "##### score threshold\n",
    "score_mask = (scores_pred > cfg.TEST_SCORE_THRESH)\n",
    "scores_pred = scores_pred[score_mask]\n",
    "proposals_pred = proposals_pred[score_mask]\n",
    "semantic_id = semantic_id[score_mask]\n",
    "\n",
    "##### npoint threshold\n",
    "proposals_pointnum = proposals_pred.sum(1)\n",
    "npoint_mask = (proposals_pointnum > cfg.TEST_NPOINT_THRESH)\n",
    "scores_pred = scores_pred[npoint_mask]\n",
    "proposals_pred = proposals_pred[npoint_mask]\n",
    "semantic_id = semantic_id[npoint_mask]\n",
    "\n",
    "##### nms\n",
    "if semantic_id.shape[0] == 0:\n",
    "    pick_idxs = np.empty(0)\n",
    "else:\n",
    "    proposals_pred_f = proposals_pred.float()  # (nProposal, N), float, cuda\n",
    "    intersection = torch.mm(proposals_pred_f, proposals_pred_f.t())  # (nProposal, nProposal), float, cuda\n",
    "    proposals_pointnum = proposals_pred_f.sum(1)  # (nProposal), float, cuda\n",
    "    proposals_pn_h = proposals_pointnum.unsqueeze(-1).repeat(1, proposals_pointnum.shape[0])\n",
    "    proposals_pn_v = proposals_pointnum.unsqueeze(0).repeat(proposals_pointnum.shape[0], 1)\n",
    "    cross_ious = intersection / (proposals_pn_h + proposals_pn_v - intersection)\n",
    "    pick_idxs = non_max_suppression(cross_ious.cpu().numpy(), scores_pred.cpu().numpy(), cfg.TEST_NMS_THRESH)  # int, (nCluster, N)\n",
    "clusters = proposals_pred[pick_idxs]\n",
    "cluster_scores = scores_pred[pick_idxs]\n",
    "cluster_semantic_id = semantic_id[pick_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_offsets_img = inpaint_with_valid_mask(pt_offsets[batch['offsets'][img_num] : batch['offsets'][img_num+1]],\n",
    "                                         valid_mask)\n",
    "semantic_pred_img = inpaint_with_valid_mask(semantic_pred[batch['offsets'][img_num] : batch['offsets'][img_num+1]],\n",
    "                                            valid_mask)\n",
    "instances_img = inpaint_cluster_img(clusters[:, batch['offsets'][img_num] : batch['offsets'][img_num+1]],\n",
    "                                   valid_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pred_instances = len([x for x in np.unique(instances_img) if x not in [0,1]])\n",
    "fg_mask = semantic_pred_img == 2\n",
    "\n",
    "plot_dict = {\n",
    "    'Point Offsets' : flowlib.flow_to_image(pt_offsets_img * fg_mask[..., None]),\n",
    "    'Semantic Prediction' : semantic_pred_img,\n",
    "    f\"Instances Prediction. Num: {num_pred_instances}\" : instances_img,\n",
    "}\n",
    "if 'instance_labels' in batch:\n",
    "    num_gt_instances = len([x for x in np.unique(gt_instances_img) if x not in [0,1]])\n",
    "    plot_dict[f\"Instances GT. Num: {num_gt_instances}\"] = gt_instances_img\n",
    "\n",
    "subplotter(plot_dict, max_plots_per_row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subplotter([instances_img == x for x in np.unique(instances_img)[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pointgroup]",
   "language": "python",
   "name": "conda-env-pointgroup-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
