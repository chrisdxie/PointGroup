GENERAL:
  task: train  # train, test
  manual_seed: 123
  model_dir: model/pointgroup/pointgroup.py
  dataset_dir: data/TOD.py

DATA:
  data_root: /data/tabletop_dataset_v5/
  dataset: TOD

  classes: 3
  ignore_label: -100

  input_channel: 3
  scale: 250   # voxel_size = 1 / scale, scale 50(2cm)
  batch_size: 2
  full_scale: [640, 2560]  # Original values were [128, 512]. Multiplied by 5
  max_npoint: 250000
  mode: 4 # 4=mean

STRUCTURE:
  model_name: pointgroup
  m: 16 # 16 or 32
  block_residual: True
  block_reps: 2

  use_coords: True

TRAIN:
  max_epochs: 10
  max_iters: 150000
  train_workers: 2 # data loader workers
  optim: Adam # Adam or SGD
  lr: 0.001
  step_epoch: 50 # not used
  multiplier: 0.5
  momentum: 0.9
  weight_decay: 0.0001
  save_freq: 1  # also eval_freq
  loss_weight: [1.0, 1.0, 1.0, 1.0] # semantic_loss, offset_norm_loss, offset_dir_loss, score_loss

  fg_thresh: 0.75
  bg_thresh: 0.25

  score_scale: 250 # the minimal voxel size is 4mm. This is 5x smaller than PointGroup (on ScanNet), which uses 2cm
  score_fullscale: 70 # Original value (ScanNet) was 14, 5x means 70.
  score_mode: 4 # mean

  pretrain_path:
  pretrain_module: []
  fix_module: []

  gamma_shape: 1000.
  gamma_scale: 0.001

  gaussian_scale_range : [0., 0.003]  # up to 3mm standard dev
  gp_rescale_factor_range : [12, 20]  # [low, high (exclusive)]

GROUP:
  ### point grouping
  cluster_radius: 0.006  # Original value was 0.03. Divided by 5
  cluster_meanActive: 50
  cluster_shift_meanActive: 300
  cluster_npoint_thre: 50

  prepare_iters: 50000

TEST:
  split: val
  test_epoch: 384
  test_workers: 16
  test_seed: 567

  TEST_NMS_THRESH: 0.3
  TEST_SCORE_THRESH: 0.09
  TEST_NPOINT_THRESH: 100

  eval: True
  save_semantic: False
  save_pt_offsets: False
  save_instance: False

