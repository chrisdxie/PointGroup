{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" # for debugging GPU stuff\n",
    "import time, random\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config file\n",
    "cfg_file = '/home/chrisxie/local_installations/PointGroup/config/pointgroup_TOD.yaml'\n",
    "from util.config import get_parser_notebook\n",
    "get_parser_notebook(cfg_file=cfg_file, pretrain_path=None)\n",
    "\n",
    "from util.config import cfg\n",
    "from util.log import logger\n",
    "import util.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init():\n",
    "    # copy important files to backup\n",
    "    backup_dir = os.path.join(cfg.exp_path, 'backup_files')\n",
    "    os.makedirs(backup_dir, exist_ok=True)\n",
    "    os.system('cp train.py {}'.format(backup_dir))\n",
    "    os.system('cp {} {}'.format(cfg.model_dir, backup_dir))\n",
    "    os.system('cp {} {}'.format(cfg.dataset_dir, backup_dir))\n",
    "    os.system('cp {} {}'.format(cfg.config, backup_dir))\n",
    "\n",
    "    # log the config\n",
    "    logger.info(cfg)\n",
    "\n",
    "    # summary writer\n",
    "    global writer\n",
    "    writer = SummaryWriter(cfg.exp_path)\n",
    "\n",
    "    # random seed\n",
    "    random.seed(cfg.manual_seed)\n",
    "    np.random.seed(cfg.manual_seed)\n",
    "    torch.manual_seed(cfg.manual_seed)\n",
    "    torch.cuda.manual_seed_all(cfg.manual_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, model_fn, optimizer, epoch):\n",
    "    iter_time = utils.AverageMeter()\n",
    "    data_time = utils.AverageMeter()\n",
    "    am_dict = {}\n",
    "\n",
    "    current_iter = (epoch - 1) * len(train_loader)\n",
    "    \n",
    "    model.train()\n",
    "    start_epoch = time.time()\n",
    "    end = time.time()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        if current_iter >= cfg.max_iters:\n",
    "            break\n",
    "        \n",
    "        ##### adjust learning rate\n",
    "        utils.step_learning_rate(optimizer, cfg.lr, epoch - 1, cfg.step_epoch, cfg.multiplier)\n",
    "\n",
    "        ##### prepare input and forward\n",
    "        loss, _, visual_dict, meter_dict = model_fn(batch, model, current_iter)\n",
    "\n",
    "        ##### meter_dict\n",
    "        for k, v in meter_dict.items():\n",
    "            if k not in am_dict.keys():\n",
    "                am_dict[k] = utils.AverageMeter()\n",
    "            am_dict[k].update(v[0], v[1])\n",
    "\n",
    "        ##### backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ##### time and print\n",
    "        current_iter += 1\n",
    "        remain_iter = cfg.max_iters - current_iter\n",
    "\n",
    "        iter_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        remain_time = remain_iter * iter_time.avg\n",
    "        t_m, t_s = divmod(remain_time, 60)\n",
    "        t_h, t_m = divmod(t_m, 60)\n",
    "        remain_time = '{:02d}:{:02d}:{:02d}'.format(int(t_h), int(t_m), int(t_s))\n",
    "\n",
    "        sys.stdout.write(\n",
    "            \"epoch: {}/{} iter: {}/{} loss: {:.4f}({:.4f}) data_time: {:.2f}({:.2f}) iter_time: {:.2f}({:.2f}) remain_time: {remain_time}\\n\".format\n",
    "            (epoch, cfg.max_epochs, i + 1, len(train_loader), am_dict['loss'].val, am_dict['loss'].avg,\n",
    "             data_time.val, data_time.avg, iter_time.val, iter_time.avg, remain_time=remain_time))\n",
    "        if (i == len(train_loader) - 1): print()\n",
    "\n",
    "    logger.info(\"epoch: {}/{}, train loss: {:.4f}, time: {}s\".format(epoch, cfg.max_epochs, am_dict['loss'].avg, time.time() - start_epoch))\n",
    "\n",
    "    utils.checkpoint_save(model, cfg.exp_path, cfg.config.split('/')[-1][:-5], epoch, cfg.save_freq, use_cuda)\n",
    "\n",
    "    for k in am_dict.keys():\n",
    "        if k in visual_dict.keys():\n",
    "            writer.add_scalar(k+'_train', am_dict[k].avg, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### init\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### get model version and data version\n",
    "exp_name = cfg.config.split('/')[-1][:-5]\n",
    "print(exp_name)\n",
    "model_name = exp_name.split('_')[0]\n",
    "print(model_name)\n",
    "data_name = exp_name.split('_')[-1]\n",
    "print(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### model\n",
    "logger.info('=> creating model ...')\n",
    "if model_name == 'pointgroup':\n",
    "    from model.pointgroup.pointgroup import PointGroup as Network\n",
    "    from model.pointgroup.pointgroup import model_fn_decorator\n",
    "else:\n",
    "    print(\"Error: no model - \" + model_name)\n",
    "    exit(0)\n",
    "model = Network(cfg)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "logger.info('cuda available: {}'.format(use_cuda))\n",
    "assert use_cuda\n",
    "model = model.cuda()\n",
    "\n",
    "# logger.info(model)\n",
    "logger.info('#classifier parameters: {}'.format(sum([x.nelement() for x in model.parameters()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### optimizer\n",
    "logger.info('=> creating optimizer ...')\n",
    "if cfg.optim == 'Adam':\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=cfg.lr)\n",
    "elif cfg.optim == 'SGD':\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=cfg.lr, momentum=cfg.momentum, weight_decay=cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### model_fn (criterion)\n",
    "model_fn = model_fn_decorator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### dataset\n",
    "if cfg.dataset == 'TOD':\n",
    "    if data_name == 'TOD':\n",
    "        import data.TOD\n",
    "        data.TOD = reload(data.TOD)\n",
    "        dataset = data.TOD.Dataset()\n",
    "        dataset.trainLoader()\n",
    "    else:\n",
    "        print(\"Error: no data loader - \" + data_name)\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### resume\n",
    "start_epoch = utils.checkpoint_restore(model,\n",
    "                                       cfg.exp_path,\n",
    "                                       cfg.config.split('/')[-1][:-5],\n",
    "                                       use_cuda)\n",
    "# resume from the latest epoch, or specify the epoch to restore\n",
    "print(f'Start epoch: {start_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### train and val\n",
    "for epoch in range(start_epoch, cfg.max_epochs + 1):\n",
    "    train_epoch(dataset.train_data_loader, model, model_fn, optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate statistics of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "distances = []\n",
    "\n",
    "num_exs = 1000\n",
    "batch_size = 5\n",
    "for i in tqdm(range(0, num_exs, batch_size)):\n",
    "    temp = dataset.train_collate_fn([i])\n",
    "    fg_mask = temp['labels'] >= 0\n",
    "    distances = distances + np.linalg.norm(temp['locs_float'][fg_mask] -\n",
    "                                           temp['instance_info'][fg_mask][:,:3], axis=-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.quantile(distances, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dataset.train_collate_fn([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xyz = temp['locs'].numpy().copy()[:,1:]\n",
    "xyz -= xyz.min(0)\n",
    "\n",
    "xyz_offset = xyz.copy()\n",
    "valid_idxs = (xyz_offset.min(1) >= 0)\n",
    "assert valid_idxs.sum() == xyz.shape[0]\n",
    "\n",
    "full_scale = np.array([cfg.full_scale[1]] * 3)\n",
    "room_range = xyz.max(0) - xyz.min(0)\n",
    "while (valid_idxs.sum() > 250000):\n",
    "    offset = np.clip(full_scale - room_range + 0.001, None, 0) * np.random.rand(3)\n",
    "    xyz_offset = xyz + offset\n",
    "    valid_idxs = (xyz_offset.min(1) >= 0) * ((xyz_offset < full_scale).sum(1) == 3)\n",
    "    full_scale[:2] -= 32\n",
    "\n",
    "xyz_offset, valid_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.count_nonzero(valid_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['v2p_map'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['v2p_map'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['p2v_map'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temp['v2p_map'].shape, temp['v2p_map'].numpy().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['locs'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(temp['labels'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(temp['instance_labels'].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(1, figsize=(10,5))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "temp_ = temp['labels'].numpy().copy().reshape(480,640)\n",
    "# temp_[temp_ == -100] = 0\n",
    "plt.imshow(temp_)\n",
    "plt.title('Seg Labels')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "temp_ = temp['instance_labels'].numpy().copy().reshape(480,640) + 1\n",
    "temp_[temp_ == -100+1] = 0\n",
    "plt.imshow(temp_)\n",
    "plt.title('Instance Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['locs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['voxel_locs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['locs_float'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp['feats'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.is_tensor(temp['feats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in temp:\n",
    "    print(key, temp[key].shape if torch.is_tensor(temp[key]) else type(temp[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pointgroup]",
   "language": "python",
   "name": "conda-env-pointgroup-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
